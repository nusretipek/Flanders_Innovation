{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Packages\n",
    "\n",
    "#Web Scraping Packages\n",
    "import requests\n",
    "from requests.utils import requote_uri\n",
    "from fake_useragent import UserAgent\n",
    "from lxml import html\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Language Detection\n",
    "import langdetect\n",
    "from textblob import TextBlob\n",
    "\n",
    "#Miscellaneous Packages\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def language_switcher(URL, lang_code):\n",
    "    success_boolean = False\n",
    "    try: \n",
    "        page = requests.get(URL)\n",
    "    except: \n",
    "        return success_boolean, ''\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    returned_list = soup.find_all(hreflang=re.compile(lang_code), href=True)\n",
    "    if (len(returned_list) == 0):\n",
    "        returned_list = soup.find_all(href=True)\n",
    "        for item in returned_list:\n",
    "            lower_string = str(item.text).lower()\n",
    "            if(any(['nl' == word for word in lower_string.split()])):\n",
    "                success_boolean = True\n",
    "                new_page = item['href']\n",
    "                if('http' not in item['href']):\n",
    "                    new_page = URL + item['href'].strip('.')\n",
    "                if language_detector(new_page)[1] == 'nl':\n",
    "                    return success_boolean, new_page \n",
    "        for item in returned_list:\n",
    "            lower_string = str(item['href']).lower()\n",
    "            if(lower_string.find('nl') != -1):\n",
    "                success_boolean = True\n",
    "                new_page = item['href']\n",
    "                if('http' not in item['href']):\n",
    "                    new_page = URL + item['href'].strip('.')\n",
    "                if language_detector(new_page)[1] == 'nl':\n",
    "                    return success_boolean, new_page         \n",
    "        return success_boolean, ''    \n",
    "    elif (len(returned_list) == 1):\n",
    "        success_boolean = True\n",
    "        new_page = returned_list[0]['href']\n",
    "        if('http' not in returned_list[0]['href']):\n",
    "            new_page = URL + returned_list[0]['href'].strip('.')\n",
    "        if language_detector(new_page)[1] == 'nl':\n",
    "            return success_boolean, new_page \n",
    "    elif (len(returned_list) > 1):\n",
    "        success_boolean = True    \n",
    "        for item in returned_list:\n",
    "            new_page = item['href']\n",
    "            if(item['href'].find('be') != -1):\n",
    "                if('http' not in item['href']):\n",
    "                    new_page = URL + item['href'].strip('.')\n",
    "                if language_detector(new_page)[1] == 'nl':\n",
    "                    return success_boolean, new_page\n",
    "        new_page = returned_list[0]['href']\n",
    "        if('http' not in returned_list[0]['href']):\n",
    "            new_page = URL + returned_list[0]['href'].strip('.')\n",
    "        if language_detector(new_page)[1] == 'nl':\n",
    "            return success_boolean, new_page      \n",
    "    else:\n",
    "        return success_boolean, ''\n",
    "    \n",
    "def visible_texts(soup):\n",
    "    re_spaces = re.compile(r'\\s{3,}')\n",
    "    text = ' '.join([s for s in soup.strings if s.parent.name not in ('style', 'script', 'head', 'title')])\n",
    "    return re_spaces.sub(' ', text)\n",
    "\n",
    "def language_detector(URL):\n",
    "    try: \n",
    "        page = requests.get(URL, timeout=10)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        for tag in soup.find_all('div', id=re.compile(r'(cook)|(popup)')):\n",
    "            tag.decompose()\n",
    "        for tag in soup.find_all('div', class_=re.compile(r'(cook)|(popup)')):\n",
    "            tag.decompose()\n",
    "        body_text = visible_texts(BeautifulSoup(visible_texts(soup), 'html.parser'))\n",
    "        if len(soup.find_all('frame')) > 0:\n",
    "            frame_text = ''\n",
    "            for f in soup.find_all('frame'):\n",
    "                frame_request = requests.get(f['src'])\n",
    "                frame_soup =  BeautifulSoup(frame_request.content, 'html.parser')\n",
    "                frame_text = frame_text + ' ' + visible_texts(BeautifulSoup(visible_texts(frame_soup), 'html.parser'))\n",
    "            body_text = body_text + frame_text\n",
    "        print(body_text)\n",
    "        return len(body_text.split()), TextBlob(body_text).detect_language()\n",
    "    except:\n",
    "        return 0, 'unknown'\n",
    "\n",
    "def crawl_contact_page(URL, Base_URL, request_page):\n",
    "    new_pages = []\n",
    "    soup_crawl = BeautifulSoup(request_page.text, 'html.parser')\n",
    "    returned_list = soup_crawl.find_all(href=True)        \n",
    "    for item in returned_list:\n",
    "        lower_href_text = ''.join(str(item.text).lower().strip())\n",
    "        if('cont' in lower_href_text):\n",
    "            if('www' in item['href']):\n",
    "                new_pages.append(item['href'])\n",
    "            else:\n",
    "                new_page = Base_URL + item['href'].strip('.')\n",
    "                new_pages.append(new_page)\n",
    "    return list(set(new_pages))\n",
    "\n",
    "def crawl_location_page(URL, Base_URL, request_page):\n",
    "    new_pages = []\n",
    "    soup_crawl = BeautifulSoup(request_page.text, 'html.parser')\n",
    "    returned_list = soup_crawl.find_all(href=True)        \n",
    "    for item in returned_list:\n",
    "        lower_href_text = ''.join(str(item.text).lower().strip())\n",
    "        if(('vest' in lower_href_text) | ('loc' in lower_href_text)):\n",
    "            if('www' in item['href']):\n",
    "                new_pages.append(item['href'])\n",
    "            else:\n",
    "                new_page = Base_URL + item['href'].strip('.')\n",
    "                new_pages.append(new_page)\n",
    "    return list(set(new_pages))\n",
    "\n",
    "def validate_zip(URL, Base_URL, zip_1, zip_2):\n",
    "    page = requests.get(URL)\n",
    "    contact_pages = crawl_contact_page(URL, Base_URL, page)\n",
    "    location_pages = crawl_location_page(URL, Base_URL, page)\n",
    "    total_pages = contact_pages + location_pages\n",
    "    print(total_pages)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    [s.decompose() for s in soup('script')]\n",
    "    all_text = ' '.join(re.sub(r'\\n', ' ', soup.get_text()).split())\n",
    "    numeric_text = re.findall(r'\\d+', all_text)\n",
    "    if (any([str(zip_1) == number for number in numeric_text]) | \n",
    "        any([str(zip_2) == number for number in numeric_text])):\n",
    "        return True\n",
    "    elif (len(total_pages) != 0):\n",
    "        for new_page in total_pages:\n",
    "            time.sleep(3)\n",
    "            page = requests.get(new_page)\n",
    "            soup = BeautifulSoup(page.text, 'lxml')\n",
    "            [s.decompose() for s in soup('script')]\n",
    "            all_text = ' '.join(re.sub(r'\\n', ' ', soup.get_text()).split())\n",
    "            numeric_text = re.findall(r'\\d+', all_text)\n",
    "            if (any([str(zip_1) == number for number in numeric_text]) | \n",
    "                any([str(zip_2) == number for number in numeric_text])):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def validate_street(URL, Base_URL, street_raw):\n",
    "    page = requests.get(URL)\n",
    "    contact_pages = crawl_contact_page(URL, Base_URL, page)\n",
    "    location_pages = crawl_location_page(URL, Base_URL, page)\n",
    "    total_pages = contact_pages + location_pages\n",
    "    print(total_pages)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    [s.decompose() for s in soup('script')]\n",
    "    all_text = ' '.join(re.sub(r'\\n', ' ', soup.get_text()).split())\n",
    "    street_raw_temp = re.sub(r'\\d+', '', street_raw).strip()\n",
    "    final_street = re.sub('[\\(\\[].*?[\\)\\]]', '', street_raw_temp) \n",
    "    if(final_street in all_text):\n",
    "        return True\n",
    "    elif (len(total_pages) != 0):\n",
    "        for new_page in total_pages:\n",
    "            time.sleep(3)\n",
    "            page = requests.get(new_page)\n",
    "            soup = BeautifulSoup(page.text, 'lxml')\n",
    "            [s.decompose() for s in soup('script')]\n",
    "            all_text = ' '.join(re.sub(r'\\n', ' ', soup.get_text()).split())\n",
    "            if(final_street in all_text):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def extract_url_from_email(Email):\n",
    "    try:\n",
    "        return (re.findall(r'@([A-Za-z.]+)', Email)[0]).strip()\n",
    "    except: \n",
    "        return ''\n",
    "    \n",
    "def fix_http(URL):\n",
    "    if URL != '':\n",
    "        if('http' in URL) & (URL[-1:] == '/'):\n",
    "            return URL\n",
    "        elif ('http' in URL) & (URL[-1:] != '/'):\n",
    "            return URL + '/'\n",
    "        elif ('http' not in URL) & (URL[-1:] == '/'):\n",
    "            return 'http://' + URL\n",
    "        else:\n",
    "            return 'http://' + URL + '/'\n",
    "    \n",
    "#Input is 4 columns; cur_email,cur_web,email,web columns\n",
    "def assign_primary_URL(cur_web, cur_email, web, email):\n",
    "    if not (pd.isnull(cur_web)):\n",
    "        return fix_http(cur_web)\n",
    "    elif not (pd.isnull(cur_email)):\n",
    "        return fix_http(extract_url_from_email(cur_email))\n",
    "    elif not (pd.isnull(web)):\n",
    "        return fix_http(web)\n",
    "    elif not (pd.isnull(email)):\n",
    "        return fix_http(extract_url_from_email(email))\n",
    "    else: \n",
    "        return ''    \n",
    "def get_status_code(URL):\n",
    "    try:\n",
    "        return requests.get(URL, timeout=10).status_code\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def get_NL_URL(URL, status_code):\n",
    "    try: \n",
    "        if status_code == 200:\n",
    "            if language_detector(URL)[1] != 'nl':\n",
    "                success_code, new_url = language_switcher(URL, 'nl')\n",
    "                if success_code & (new_url != ''):\n",
    "                    return new_url\n",
    "        return URL\n",
    "    except:\n",
    "        return URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.biagroup.com/bel-vl/'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_NL_URL(\"http://www.biagroup.com/\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('http://www.farys.be/', timeout=10).status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'C:\\\\Users\\\\nusret\\\\Desktop\\\\Thesis\\\\Data\\\\inno5_address_toStatVla_cis19.xlsx'\n",
    "CIS_Survey_Dataframe = pd.read_excel(FILE, sheet_name='inno5_address_toStatVla_cis19')\n",
    "CIS_Survey_Dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a best guess URL\n",
    "pd.options.mode.chained_assignment = None  \n",
    "CIS_Survey_Dataframe['best_guess'] = CIS_Survey_Dataframe.apply(lambda x: assign_primary_URL(x.cur_web, x.cur_email, x.web, x.email), axis=1)\n",
    "CIS_Survey_Dataframe[100:130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3179/3179 [53:33<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "tqdm(disable=True, total=0)\n",
    "if len(tqdm._instances) > 0:\n",
    "    while len(tqdm._instances) > 0:\n",
    "        tqdm._instances.pop().close()\n",
    "    clear_output(wait=True)\n",
    "\n",
    "CIS_Survey_Dataframe['status_code'] = 0  \n",
    "\n",
    "for index, row in tqdm(CIS_Survey_Dataframe.iterrows(), total=CIS_Survey_Dataframe.shape[0]):\n",
    "    CIS_Survey_Dataframe.loc[index,'status_code'] = get_status_code(row['best_guess'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIS_Survey_Dataframe[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200    2390\n",
       "0       685\n",
       "403      72\n",
       "404      15\n",
       "500       7\n",
       "503       6\n",
       "406       3\n",
       "999       1\n",
       "Name: status_code, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIS_Survey_Dataframe['status_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIS_Survey_Dataframe.to_pickle('CIS_Status_Code.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIS_Survey_Dataframe = pd.read_pickle('CIS_Status_Code.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIS_200 = CIS_Survey_Dataframe[CIS_Survey_Dataframe['status_code'] == 200]\n",
    "CIS_non_200 = CIS_Survey_Dataframe[CIS_Survey_Dataframe['status_code'] != 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  \n",
    "tqdm(disable=True, total=0)\n",
    "if len(tqdm._instances) > 0:\n",
    "    while len(tqdm._instances) > 0:\n",
    "        tqdm._instances.pop().close()\n",
    "    clear_output(wait=True)\n",
    "\n",
    "CIS_200['NL_URL'] = 0  \n",
    "\n",
    "for index, row in tqdm(CIS_200.iterrows(), total=CIS_200.shape[0]):\n",
    "    CIS_200.loc[index,'NL_URL'] = get_NL_URL(row['best_guess'], row['status_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIS_200.to_pickle('CIS_200NL.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIS_200[1600:1650]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████▍    | 2103/2390 [49:21<06:22,  1.33s/it]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "100%|██████████████████████████████████████| 2390/2390 [55:15<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None  \n",
    "tqdm(disable=True, total=0)\n",
    "if len(tqdm._instances) > 0:\n",
    "    while len(tqdm._instances) > 0:\n",
    "        tqdm._instances.pop().close()\n",
    "    clear_output(wait=True)\n",
    "\n",
    "CIS_200['language'] = 0  \n",
    "\n",
    "for index, row in tqdm(CIS_200.iterrows(), total=CIS_200.shape[0]):\n",
    "    CIS_200.loc[index,'language'] = language_detector(row['NL_URL'])[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nl         1646\n",
       "en          597\n",
       "unknown      81\n",
       "fr           40\n",
       "de           12\n",
       "lb            5\n",
       "co            2\n",
       "hu            1\n",
       "pl            1\n",
       "zh-TW         1\n",
       "pt            1\n",
       "af            1\n",
       "ja            1\n",
       "sv            1\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIS_200.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIS_200.to_pickle('CIS_200NL.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIS_Survey_Dataframe_NL = pd.read_pickle('CIS_200NL.pkl')\n",
    "CIS_Survey_Dataframe_NON200 = pd.read_pickle('CIS_non_200.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      685\n",
       "403     72\n",
       "404     15\n",
       "500      7\n",
       "503      6\n",
       "406      3\n",
       "999      1\n",
       "Name: status_code, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIS_Survey_Dataframe_NON200.status_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_websites = pd.read_csv(\"C:\\\\Users\\\\nusret\\\\Desktop\\\\Thesis\\\\Data\\\\Manual Found Merge\\\\Scrape Merge_CSV.csv\", sep =';')\n",
    "CIS_Survey_Dataframe_Merged = pd.merge(CIS_Survey_Dataframe_NON200,\n",
    "                                       manual_websites,\n",
    "                                       left_on = 'btw',\n",
    "                                       right_on = 'btw',\n",
    "                                       how = 'left')\n",
    "CIS_Survey_Dataframe_Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in CIS_Survey_Dataframe_Merged.iterrows():\n",
    "    if(CIS_Survey_Dataframe_Merged.loc[index,'best_guess'] == \"\"):\n",
    "            CIS_Survey_Dataframe_Merged.loc[index,'best_guess'] = fix_http(str(CIS_Survey_Dataframe_Merged.loc[index,'Found Website']))\n",
    "CIS_Survey_Dataframe_Merged = CIS_Survey_Dataframe_Merged.iloc[:, :-2]\n",
    "CIS_Survey_Dataframe_Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 789/789 [22:12<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "tqdm(disable=True, total=0)\n",
    "if len(tqdm._instances) > 0:\n",
    "    while len(tqdm._instances) > 0:\n",
    "        tqdm._instances.pop().close()\n",
    "    clear_output(wait=True)\n",
    "\n",
    "CIS_Survey_Dataframe_Merged['status_code'] = 0  \n",
    "\n",
    "for index, row in tqdm(CIS_Survey_Dataframe_Merged.iterrows(), total=CIS_Survey_Dataframe_Merged.shape[0]):\n",
    "    CIS_Survey_Dataframe_Merged.loc[index,'status_code'] = get_status_code(row['best_guess'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIS_Survey_Dataframe_Merged.to_pickle('Merged_Final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      340\n",
       "200    330\n",
       "403     80\n",
       "404     22\n",
       "503      6\n",
       "500      6\n",
       "406      4\n",
       "999      1\n",
       "Name: status_code, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIS_Survey_Dataframe_Merged.status_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200    2390\n",
       "Name: status_code, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIS_Survey_Dataframe_NL.status_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|█████████████████████████████████▍      | 659/789 [08:04<03:04,  1.42s/it]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "100%|████████████████████████████████████████| 789/789 [09:38<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None  \n",
    "tqdm(disable=True, total=0)\n",
    "if len(tqdm._instances) > 0:\n",
    "    while len(tqdm._instances) > 0:\n",
    "        tqdm._instances.pop().close()\n",
    "    clear_output(wait=True)\n",
    "\n",
    "CIS_Survey_Dataframe_Merged['NL_URL'] = 0  \n",
    "\n",
    "for index, row in tqdm(CIS_Survey_Dataframe_Merged.iterrows(), total=CIS_Survey_Dataframe_Merged.shape[0]):\n",
    "    CIS_Survey_Dataframe_Merged.loc[index,'NL_URL'] = get_NL_URL(row['best_guess'], row['status_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████▉                      | 353/789 [11:00<24:28,  3.37s/it]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "100%|████████████████████████████████████████| 789/789 [22:35<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None  \n",
    "tqdm(disable=True, total=0)\n",
    "if len(tqdm._instances) > 0:\n",
    "    while len(tqdm._instances) > 0:\n",
    "        tqdm._instances.pop().close()\n",
    "    clear_output(wait=True)\n",
    "\n",
    "CIS_Survey_Dataframe_Merged['language'] = 0  \n",
    "\n",
    "for index, row in tqdm(CIS_Survey_Dataframe_Merged.iterrows(), total=CIS_Survey_Dataframe_Merged.shape[0]):\n",
    "    CIS_Survey_Dataframe_Merged.loc[index,'language'] = language_detector(row['NL_URL'])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unknown    350\n",
       "nl         268\n",
       "en         166\n",
       "fr           3\n",
       "id           1\n",
       "de           1\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIS_Survey_Dataframe_Merged.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIS_Survey_Dataframe_Merged = pd.read_pickle('data/Merged_Final.pkl')\n",
    "CIS_Survey_Dataframe_NL200 = pd.read_pickle('data/CIS_200NL.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIS_Survey_Dataframe_NL200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['btw', 'cur_firm', 'groupStata19', 'gpnameStata19', 'inno5',\n",
       "       'cur_street1', 'cur_street2', 'finalzip', 'nuts', 'regionRsp',\n",
       "       'zipActivPost19', 'nutsActivPost19', 'regionactivPost19', 'cur_email',\n",
       "       'cur_web', 'street1', 'street2', 'email', 'web', 'best_guess',\n",
       "       'status_code', 'NL_URL', 'language'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIS_Survey_Dataframe_Merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [CIS_Survey_Dataframe_NL200, CIS_Survey_Dataframe_Merged]\n",
    "CIS_Survey_Dataframe_Final = pd.concat(frames).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIS_Survey_Dataframe_Final\n",
    "Excel_CIS_Manual_Check = CIS_Survey_Dataframe_Final[['btw', 'index', 'cur_firm', 'best_guess', 'status_code', 'NL_URL', 'language']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Excel_CIS_Manual_Check.to_excel(\"Manual_Check_SheetBTW.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 'unknown')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_detector(\"http://arcelormittal.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent()\n",
    "header = {'User-Agent':str(ua.random)}\n",
    "page = requests.get(\"https://tokheim.com/\", timeout=10, headers=header)\n",
    "page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Contacteer ons menu Language Global - English Benelux - Français Benelux - Nederlands Canada - English Canada - Français Deutschland - Deutsch España - Español France - Français India - English Italia – Italiano Singapore - English Türkiye - Türkçe UK - English USA - English 日本 - 日本語 中国 - 简体中文 대한민국 - 한국어 Vakgebieden Producten Oplossingen In de praktijk Duurzaamheid Over DIGI DIGI Benelux, gevestigd te Antwerpen, is een dynamisch en groeiend bedrijf dat weegschalen-, prijs- en etiketteersystemen produceert. Als marktleider leveren ze o.a. aan de retail-, industrie- en voedingsmiddelenbranche. DIGI biedt toonaangevende oplossingen voor de voedingsindustrie met een breed gamma aan producten. DIGI biedt uitgebreide oplossingen die de productiviteit verhogen in de logistieke sector. Ontdek onze weegschalen en verpakkingsoplossingen specifiek voor de logistiek! DIGI biedt innovatieve oplossingen die de activiteiten stroomlijnen en de winst vergroten in de horeca. DIGI ondersteunt al zijn producten met een eigen servicedienst. Ofwel bij u op het bedrijf of van op afstand. Digi heeft ook een uitgebreide ijkbevoegdheid. Retail Industrie Logistiek Horeca Service & Cloud Producten SM-6000 De toonbankweegschaal voor de toekomst T@POP ESL Herdefinieer eenvoud AW-5600ATII Volledig geïntegreerd weeg-inpak-etiket systeem ontworpen voor operationeel comfort en milieuvriendelijkheid SM-120LL Entry-level digitale weegschaal! AW-5600FXII Dankzij de nieuwste upgrade worden moeilijk-te-verpakken items met gemak getackeld DPS-5600II Krachtige pre-pack weegschaal printer Meer producten Oplossingen Oplossing voor productieomgeving Intelligente weegschalen zorgen voor een aangename self-service ervaring. T@POP Solution Dynamische en flexibele prijsupdates Verpakoplossing Een ruime keuze aan verpakoplossingen Oplossing met lijnloze etiketten Lijnloze etikketen voorzien variabele lengtes en afdrukformaten voor uw winkel Etiketbeheer Cloud Oplossing Maak uw etiketten op het hoofdkantoor. Verdeel ze nadien allemaal online. Verbetering Productiviteit Een intelligente oplossing voor uw voedingstoonbank Meer oplossingen Klant case studies Kaaskenner  (Netherlands) First Pilot Cheese Connoisseur Store opens in the Netherlands with SM-6000 Renmans  (Belgium, France, Luxemburg) Renmans kiest voor SM5500G om cash betaalautomaten te integreren. Ryan’s SuperValu Glanmire  (Ireland) SuperValu is één van de grootste Ierse supermarktketens. De vestiging die koos voor de DIGI ESL oplossing is gelegen in Cork, de tweede grootste stad van Ierland. Meer casestudies Nieuwsberichten 14 okt, 2020 nieuwsberichten Nieuwe Case Study - DIGI geïntegreerde oplossingen verbeteren de klanttevredenheid in de grootste hypermarkt van FairPrice. 01 okt, 2020 nieuwsberichten Eerste e.Sense-installatie in Frankrijk bij Intermarché 07 sep, 2020 nieuwsberichten DIGI POS weegschaal - Krachtige partner voor uw detailhandel 04 sep, 2020 nieuwsberichten DIGI ESL - Electronic Signage Solution - Redefine Simplicity 27 aug, 2020 nieuwsberichten Nieuw product - SM-6000SSR/SSP, Zelfbedieningsweegschaal die de milieubewuste levensstijl van de consument ondersteunt Meer nieuws Informatie Go to page top Voor meer details : 32 (0)3 325 81 01 Belgium 31 (0)20-236 1300 Netherlands Contactformulier Vakgebieden Retail Industrie Logistik Horeca Service & Cloud Producten Nach Kategorie Nach Modell Suchbegriff Oplossingen Retail Industrie Logistik Horeca In de praktijk Retail Industrie Logistiek Horeca Duurzaamheid Product/Dienstenontwikkeling Systeemontwikkeling Bedrijfsontwikkeling Over DIGI Bedrijfsprofiel Bereikbaarheid Showrooms DIGI Groep Ondersteuning Verbruiksartikelen Bestel hier online uw etiketten Herijk Herijk aanvraag online Jobs Contacteer ons Schrijf mij in voor de Nieuwsbrief Privacybeleid Algemene voorwaarden Handelsmerk sitemap Land - Taal Global - English Benelux - Français Benelux - Nederlands Canada - English Canada - Français Deutschland - Deutsch España - Español France - Français India - English Italia – Italiano Singapore - English Türkiye - Türkçe UK - English USA - English 日本 - 日本語 中国 - 简体中文 대한민국 - 한국어 © Teraoka Seiko Co., Ltd. All Rights Reserved. '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ua = UserAgent()\n",
    "header = {'User-Agent':str(ua.random)}\n",
    "page = requests.get(\"https://www.digisystem.com/be/nl/\", timeout=10, headers=header)\n",
    "#print(page.text)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "visible_texts(soup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
